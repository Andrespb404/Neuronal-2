<!DOCTYPE html>
<html lang="es">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.8/dist/css/bootstrap.min.css" rel="stylesheet"
        integrity="sha384-sRIl4kxILFvY47J16cr9ZwB07vP4J8+LH7qKQnuqkuIAvNWLzeN8tE5YBujZqJLB" crossorigin="anonymous">
    <link href="https://db.onlinewebfonts.com/c/2c4768612470267e8024e88a66d43cf3?family=Corporate+S+Regular"
        rel="stylesheet">
    <link rel="stylesheet" href="assets/css/styles.css">
    <link rel="stylesheet" href="assets/css/present.css">
    <link rel="stylesheet" href="assets/css/history.css">
    <title>NeuAI</title>
</head>

<body>
    <nav class="navbar navbar-expand-lg">
        <div class="container">
            <img src="assets/img/logo.svg" width="120" alt="">
            <div class="navbar-collapse justify-content-end">
                <ul class="navbar-nav">
                    <li class="nav-item">
                        <a class="nav-link" href="index.html">Inicio</a>
                    </li>
                    <li class="nav-item">
                        <a class="nav-link" href="concepts.html">Conceptos</a>
                    </li>
                    <li class="nav-item">
                        <a class="nav-link" href="#">Modelos</a>
                    </li>
                </ul>
            </div>
            <div>
            </div>
        </div>
    </nav>

    <div class="space"></div>

    <section>
        <div class="container">
            <div class="row">
                <div class="col">
                    <p class="m-0 ps-5">La</p>
                    <div class="principal-title">
                        <h4 class="mt-1 ps-5">Historia</h4>
                    </div>
                </div>
                <div class="col">
                    <p>
                        Las redes neuronales artificiales surgieron en 1958 con el perceptrón de Frank
                        Rosenblatt, un modelo simple que inspiró el desarrollo posterior de
                        arquitecturas
                        más complejas. En 1965 apareció el perceptrón multicapa, incorporando capas
                        ocultas
                        para resolver problemas más difíciles. Durante los años 80 se introdujeron
                        funciones
                        de activación como la sigmoide, las redes feedforward y el algoritmo de
                        backpropagation, que permitió entrenar redes profundas. A finales de esa década
                        surgieron las redes convolucionales (CNN) para procesar imágenes y las redes
                        recurrentes (RNN) para secuencias, con la creación en 1997 de las LSTM para
                        mejorar
                        la memoria en tareas largas. En 2006 el campo renació con las Deep Belief
                        Networks y
                        el concepto de deep learning, impulsado por mayor capacidad de cómputo y grandes
                        volúmenes de datos. Finalmente, en 2014 las GANs revolucionaron el área al
                        generar
                        contenidos realistas, consolidando a las redes neuronales como la base de la
                        inteligencia artificial moderna.
                    </p>
                </div>
            </div>
            <div class="row pt-5">
                <div class="principal-img">
                    <img src="assets/img/Warren.jpg" alt="" width="100%" height="100%">
                </div>
            </div>
        </div>
    </section>

    <div class="space"></div>

    <section>
        <div class="container d-flex justify-content-center" style="min-height:100vh;">
            <div class="content" style="max-width:800px;">
                <h5>Historia de las Redes Neuronales</h5>

                <ol>
                    <li>
                        <h6 class="fw-bold pt-5">Primeros pasos: los orígenes (1940 – 1950)</h6>
                        <p class="pt-3">
                            En 1943, Warren McCulloch y Walter Pitts publicaron un modelo matemático inspirado en el
                            cerebro
                            humano, considerado el punto de partida de las redes neuronales artificiales. Este modelo
                            describía
                            cómo las neuronas podían representar funciones lógicas básicas.
                            En 1958, Frank Rosenblatt desarrolló el Perceptrón, la primera red neuronal capaz de
                            aprender a
                            partir de ejemplos simples.
                        </p>
                    </li>
                    <li>
                        <h6 class="fw-bold pt-5">La primera crisis: limitaciones y estancamiento (1960 – 1970)</h6>
                        <p class="pt-3">
                            A pesar del entusiasmo inicial, las limitaciones matemáticas del perceptrón —señaladas en el
                            libro
                            Perceptrons de Marvin Minsky y Seymour Papert (1969)— llevaron a una gran pérdida de
                            interés. La
                            falta de capacidad computacional y de datos suficientes también frenó la investigación.
                        </p>
                    </li>
                    <li>
                        <h6 class="fw-bold pt-5">Renacimiento con la retropropagación (1980 – 1990)</h6>
                        <p class="pt-3">
                            En los años 80, David Rumelhart, Geoffrey Hinton y Ronald Williams popularizaron el
                            algoritmo de
                            retropropagación del error (backpropagation), que permitió entrenar redes multicapa de forma
                            más
                            eficiente. Esto abrió la puerta a resolver problemas más complejos.
                        </p>
                    </li>
                    <li>
                        <h6 class="fw-bold pt-5">La segunda crisis y el aprendizaje automático (1990 – 2000)</h6>
                        <p class="pt-3">
                            Durante los 90, otras técnicas como las máquinas de soporte vectorial (SVM) y los árboles de
                            decisión ganaron terreno. Las redes neuronales quedaron en segundo plano por sus altos
                            costos de
                            entrenamiento y la falta de grandes volúmenes de datos.
                        </p>
                    </li>
                    <li>
                        <h6 class="fw-bold pt-5">El auge del Deep Learning (2000 – actualidad)</h6>
                        <p class="pt-3">
                            Con el desarrollo de procesadores gráficos (GPU), la disponibilidad de datos masivos y
                            nuevas arquitecturas, como las redes convolucionales (CNN) y las redes recurrentes (RNN),
                            las redes neuronales resurgieron.
                            A partir de 2012, con el éxito de AlexNet en la competencia ImageNet, el deep learning se
                            consolidó como la base de los grandes avances en visión por computadora, procesamiento del
                            lenguaje natural y sistemas inteligentes en general.
                        </p>
                    </li>
                    <li>
                        <h6 class="fw-bold pt-5">Perspectiva histórica</h6>
                        <p class="pt-3">
                            La evolución de las redes neuronales muestra un ciclo de entusiasmo, crisis y renacimiento,
                            impulsado por el avance tecnológico. Hoy representan una de las áreas más influyentes de la
                            inteligencia artificial moderna.
                        </p>
                    </li>
                </ol>

                <h4 class="pt-5">Bibliografía</h4>
                <ul class="pt-3">
                    <li>
                        McCulloch, W., & Pitts, W. (1943). A logical calculus of the ideas immanent in nervous activity.
                        Bulletin of Mathematical Biophysics.
                    </li>
                    <li>
                        Rosenblatt, F. (1958). The Perceptron: a probabilistic model for information storage and
                        organization in the brain. Psychological Review.
                    </li>
                    <li>
                        Minsky, M., & Papert, S. (1969). Perceptrons. MIT Press.
                    </li>
                    <li>
                        Rumelhart, D., Hinton, G., & Williams, R. (1986). Learning representations by back-propagating
                        errors. Nature.
                    </li>
                    <li>LeCun, Y., Bengio, Y., & Hinton, G. (2015). Deep learning. Nature, 521(7553), 436–444.

                    </li>


                </ul>

            </div>
        </div>
    </section>

    <div class="space"></div>

    <section>
        <div class="container">
            <div class="row">
                <div class="col">
                    <div class="final-img">
                        <img src="assets/img/history (1).jpg" alt="" width="100%" height="100%">
                    </div>
                </div>
                <div class="col">
                    <div class="final-img">
                        <img src="assets/img/history (2).jpg" alt="" width="100%" height="100%">
                    </div>
                </div>
                <div class="col">
                    <div class="final-img">
                        <img src="assets/img/brain.jpg" alt="" width="100%" height="100%">
                    </div>
                </div>
            </div>
        </div>
    </section>

    <div class="space-alt"></div>

    <div class="footer">
        <div class="container">
            <div class="col">
                <div class="row">
                    <div class="col-md-6">
                        <div class="footer-left">
                            <div class="row">
                                <div class="main-logo">
                                    <img src="assets/img/logo.svg" width="120" height="100" alt="">
                                </div>
                            </div>
                            <div class="row">
                                <div class="social-logo">
                                    <img src="assets/img/instagram.svg" width="20" alt="">
                                </div>
                            </div>
                        </div>
                    </div>
                    <div class="col-md-6">
                        <div class="footer-right">
                            <div class="row">
                                <h6>Paginas</h6>
                                <ul>
                                    <li><a href="index.html">Inicio</a></li>
                                    <li><a href="index.html">Conceptos</a></li>
                                    <li><a href="index.html">Modelos</a></li>
                                </ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            <hr class="final-line">
            <div class="row">
                <div class="col">
                    <p class="text-center pt-3">© 2024 Red Neuronales. All rights reserved.</p>
                </div>
            </div>
        </div>
    </div>
</body>